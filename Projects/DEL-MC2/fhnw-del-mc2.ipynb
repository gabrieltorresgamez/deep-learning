{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330f2133",
   "metadata": {
    "papermill": {
     "duration": 0.007268,
     "end_time": "2024-01-05T17:31:10.968863",
     "exception": false,
     "start_time": "2024-01-05T17:31:10.961595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width: 30%; float: right; margin: 10px; margin-right: 5%;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/FHNW_Logo.svg/2560px-FHNW_Logo.svg.png\" width=\"500\" style=\"float: left; filter: invert(50%);\"/>\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align: left; margin-top: 10px; float: left; width: 60%;\">\n",
    "    Mini-Challenge 2:<br>Paper-Studium und Umsetzung\n",
    "</h1>\n",
    "\n",
    "<p style=\"clear: both; text-align: left;\">\n",
    "    Bearbeitet durch Gabriel Torres Gamez im HS 2023.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dad3a4",
   "metadata": {
    "papermill": {
     "duration": 0.006437,
     "end_time": "2024-01-05T17:31:10.983829",
     "exception": false,
     "start_time": "2024-01-05T17:31:10.977392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ziel\n",
    "Für diese Mini Challenge bearbeite ich das Paper [Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2014, November 17). Show and Tell: A Neural Image Caption Generator. arXiv.Org.](https://doi.org/10.48550/arXiv.1411.4555). In diesem Paper wird ein Modell vorgestellt, das Bilder in natürliche Sprache beschreibt (Image Captioning). Es kombiniert Convolutional Neural Networks (CNN) und Long Short-Term Memory Networks (LSTM), wobei der Output des CNN mit den tokenisierten Bildbeschreibungen (Captions) konkatiniert und dann in das LSTM-Netzwerk eingespeist wird. Das Modell wird auf dem Datensatz [Hodosh, M., Young, P., & Hockenmaier, J. (2013, August 30). Flickr8k Dataset.\n",
    "](http://hockenmaier.cs.illinois.edu/8k-pictures.html) trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fb520",
   "metadata": {
    "papermill": {
     "duration": 0.006484,
     "end_time": "2024-01-05T17:31:10.996822",
     "exception": false,
     "start_time": "2024-01-05T17:31:10.990338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "Hier werden alle benötigten Bibliotheken importiert und Einstellungen vorgenommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a781644-b676-4efe-973a-7027faa1da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# 3rd Party Libraries\n",
    "import matplotlib as mpl\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as li\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize as nltk_word_tokenize\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchmetrics.text.bleu import BLEUScore\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# Print information about the environment\n",
    "print(\"Packages: \")\n",
    "print(f\" | Python Version: {sys.version}\")\n",
    "print(f\" | Matplotlib Version: {mpl.__version__}\")\n",
    "print(f\" | NLTK Version: {nltk.__version__}\")\n",
    "print(f\" | Numpy Version: {np.__version__}\")\n",
    "print(f\" | Pandas Version: {pd.__version__}\")\n",
    "print(f\" | PyTorch Lightning Version: {li.__version__}\")\n",
    "print(f\" | PyTorch Version: {torch.__version__}\")\n",
    "print(f\" | Torchmetrics Version: {torchmetrics.__version__}\")\n",
    "print(f\" | Torchvision Version: {torchvision.__version__}\")\n",
    "print(f\" | Wandb Version: {wandb.__version__}\")\n",
    "print()\n",
    "\n",
    "print(\"Backends: \")\n",
    "print(f\" | CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"   | CUDA Built: {torch.backends.cuda.is_built()}\")\n",
    "print(f\" | MPS: {torch.backends.mps.is_available()}\")\n",
    "print(f\"   | MPS Built: {torch.backends.mps.is_built()}\")\n",
    "print()\n",
    "\n",
    "print(\"Ressources: \")\n",
    "print(f\" | CPU Threads: {torch.get_num_threads()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\" | GPU {i+1}: {torch.cuda.get_device_name(i)}\\n\")\n",
    "\n",
    "# Settings\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "nltk.download('punkt')\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Silence warnings\n",
    "%env WANDB_SILENT=True\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "\n",
    "# Final message\n",
    "print(\"Setup complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81dafc3",
   "metadata": {
    "papermill": {
     "duration": 0.013103,
     "end_time": "2024-01-05T17:31:13.172334",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.159231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier werden alle benötigten Konstanten definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93bd68b",
   "metadata": {
    "papermill": {
     "duration": 0.015788,
     "end_time": "2024-01-05T17:31:13.201142",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.185354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Files\n",
    "IMG_PATH = \"data/images\"\n",
    "ANN_FILE = \"data/captions.txt\"\n",
    "\n",
    "# Data Module\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 11 # number of CPU cores - 1 | 0 on Apple Silicon\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Special Tokens\n",
    "START_TOKEN = \"<start>\"\n",
    "END_TOKEN = \"<end>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "SPECIAL_TOKENS = [START_TOKEN, END_TOKEN, PAD_TOKEN, UNK_TOKEN]\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15daa5fc",
   "metadata": {
    "papermill": {
     "duration": 0.006671,
     "end_time": "2024-01-05T17:31:13.214605",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.207934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier wird der Tokenizer definiert. Wir nehmen für diese Mini Challenge den Word Tokenizer `(nltk_word_tokenize)` von NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66adeec1",
   "metadata": {
    "papermill": {
     "duration": 0.022716,
     "end_time": "2024-01-05T17:31:13.243878",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.221162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk_word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062e086",
   "metadata": {
    "papermill": {
     "duration": 0.006692,
     "end_time": "2024-01-05T17:31:13.257252",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.250560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Arbeitsschritte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5c5d4",
   "metadata": {
    "papermill": {
     "duration": 0.006636,
     "end_time": "2024-01-05T17:31:13.270712",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.264076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Schritt 1: Daten \n",
    "In diesem Schritt lesen wir die Daten ein und bereiten sie für das Training vor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb36596",
   "metadata": {
    "papermill": {
     "duration": 0.006787,
     "end_time": "2024-01-05T17:31:13.284331",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.277544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier finden wir alle verschiedenen Captions für jedes Bild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ea06e",
   "metadata": {
    "papermill": {
     "duration": 0.044527,
     "end_time": "2024-01-05T17:31:13.335566",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.291039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read annotations as df\n",
    "ann = pd.read_csv(ANN_FILE, sep=\",\")\n",
    "ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f91e7",
   "metadata": {
    "papermill": {
     "duration": 0.006711,
     "end_time": "2024-01-05T17:31:13.349137",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.342426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier definieren wir anhand der Daten und des Tokenizers die Vokabulargrösse und die maximale Länge der Captions (mit Start- und Endtoken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88a941",
   "metadata": {
    "papermill": {
     "duration": 3.262306,
     "end_time": "2024-01-05T17:31:16.618237",
     "exception": false,
     "start_time": "2024-01-05T17:31:13.355931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tokens_set = set(ann[\"caption\"].apply(tokenizer).explode().tolist())\n",
    "\n",
    "VOCAB_SIZE = len(all_tokens_set) + len(SPECIAL_TOKENS) \n",
    "MAX_CAPTION_LENGTH = ann[\"caption\"].apply(tokenizer).apply(len).max() + 2  # + start & end token\n",
    "\n",
    "print(f\"Vocabulary Size:\\t\\t\\t{VOCAB_SIZE}\")\n",
    "print(f\"Max. Number of Tokens in a caption:\\t{MAX_CAPTION_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874ba535",
   "metadata": {
    "papermill": {
     "duration": 0.006795,
     "end_time": "2024-01-05T17:31:16.632302",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.625507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier erstellen wir ein Vokabular, welches die Tokens in Zahlen umwandelt (und umgekehrt).\n",
    "Zusätzlich erstellen wir eine Funktion, welches jede Caption mit Start- und Endtoken versieht und diese auf die maximale Länge paddet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2afe5",
   "metadata": {
    "papermill": {
     "duration": 0.014873,
     "end_time": "2024-01-05T17:31:16.653973",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.639100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocabulary which maps tokens to indices\n",
    "vocab = {token: idx for idx, token in enumerate(sorted(all_tokens_set))}\n",
    "for token in SPECIAL_TOKENS:\n",
    "    vocab[token] = len(vocab)\n",
    "    \n",
    "# inverse vocabulary which maps indices to tokens\n",
    "inv_vocab = {idx: token for token, idx in vocab.items()} # für rücktransformation\n",
    "\n",
    "# convert tokens str to int, if token not in vocab, use unk_token\n",
    "def tokens_str_to_int(tokens):\n",
    "    return [vocab.get(token, vocab[UNK_TOKEN]) for token in tokens] \n",
    "\n",
    "# converts tokens int to str, if token not in vocab, use unk_token\n",
    "def tokens_int_to_str(tokens_idx):\n",
    "    return [inv_vocab.get(idx, UNK_TOKEN) for idx in tokens_idx]\n",
    "\n",
    "# converts a string to a list of tokens with the length max_num_tokens\n",
    "def text_process(text):\n",
    "    tokenized_text = [START_TOKEN] + tokenizer(text) + [END_TOKEN]\n",
    "    n_pad = MAX_CAPTION_LENGTH - len(tokenized_text)\n",
    "    return tokenized_text + n_pad * [PAD_TOKEN]\n",
    "\n",
    "example_caption = ann[\"caption\"][0]\n",
    "print(f\"Example caption:\\n{example_caption}\\n\")\n",
    "print(f\"Tokenized caption:\\n{text_process(example_caption)}\\n\")\n",
    "print(f\"Tokenized caption (int):\\n{tokens_str_to_int(text_process(example_caption))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aba227",
   "metadata": {
    "papermill": {
     "duration": 0.00688,
     "end_time": "2024-01-05T17:31:16.667973",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.661093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier definieren wir das Preprocessing der Bilder. Wir skalieren jedes Bild auf 3x224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac0f4f",
   "metadata": {
    "papermill": {
     "duration": 0.010804,
     "end_time": "2024-01-05T17:31:16.685702",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.674898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),  # Rotates ±10 degrees\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Optional\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a916415",
   "metadata": {
    "papermill": {
     "duration": 0.00695,
     "end_time": "2024-01-05T17:31:16.699691",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.692741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier erstellen wir das DataSet, welches uns die Bilder, die Captions und die umgewandelten Captions (int) zurückgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8acd1",
   "metadata": {
    "papermill": {
     "duration": 0.011588,
     "end_time": "2024-01-05T17:31:16.718145",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.706557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define dataset\n",
    "class Flickr8kDataSet(Dataset):\n",
    "    def __init__(self, ann):\n",
    "        self.ann = ann.reset_index(drop=True)\n",
    "        self.unique_images = self.ann[\"image\"].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann[\"image\"].unique())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Image \n",
    "        image_path = os.path.join(IMG_PATH, self.unique_images[idx])\n",
    "        image = torchvision.io.read_image(image_path)\n",
    "        image = transform(image)\n",
    "\n",
    "        # Tokenized Caption as str\n",
    "        targets = self.ann[self.ann[\"image\"] == self.unique_images[idx]][\"caption\"].values\n",
    "        targets = [text_process(target) for target in targets]\n",
    "        \n",
    "        # Tokenized Caption as int\n",
    "        targets_idx = [tokens_str_to_int(target) for target in targets]\n",
    "        targets_idx = torch.tensor(targets_idx)\n",
    "        \n",
    "\n",
    "        return image, targets, targets_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11138730",
   "metadata": {
    "papermill": {
     "duration": 0.006863,
     "end_time": "2024-01-05T17:31:16.732008",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.725145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier schauen wir uns ein Beispiel an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3946cf",
   "metadata": {
    "papermill": {
     "duration": 0.136244,
     "end_time": "2024-01-05T17:31:16.875266",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.739022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_img(image, captions, stringify=True):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    for caption in captions:\n",
    "        if stringify:\n",
    "            print(f\"Caption: {' '.join([token for token in caption if token not in SPECIAL_TOKENS])}\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption}\")\n",
    "\n",
    "# get a picture and caption\n",
    "full_dataset_iter = iter(Flickr8kDataSet(ann))\n",
    "images, captions, _ = next(full_dataset_iter)\n",
    "plot_img(images, captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907980b",
   "metadata": {
    "papermill": {
     "duration": 0.00895,
     "end_time": "2024-01-05T17:31:16.893133",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.884183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Jetzt erstellen wir das DataModule. In diesem werden die Daten geladen, in Train-, Val- und Testset aufgeteilt und die DataLoader erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886073f7",
   "metadata": {
    "papermill": {
     "duration": 0.01409,
     "end_time": "2024-01-05T17:31:16.915702",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.901612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Flickr8kDataModule(li.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # split by image\n",
    "        images = ann.image.unique()\n",
    "        \n",
    "        # calculate splitsizes\n",
    "        n_images = len(images)\n",
    "        n_val_samples = int(n_images * VAL_RATIO)\n",
    "        n_test_samples = int(n_images * TEST_RATIO)\n",
    "\n",
    "        # split\n",
    "        torch.manual_seed(27112000)\n",
    "        image_split = random_split(\n",
    "            images,\n",
    "            [\n",
    "                n_images - n_val_samples - n_test_samples,\n",
    "                n_val_samples,\n",
    "                n_test_samples,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # retrieve image from split\n",
    "        train_images = images[image_split[0].indices]\n",
    "        val_images = images[image_split[1].indices]\n",
    "        test_images = images[image_split[2].indices]\n",
    "        \n",
    "        # convert to df for dataset\n",
    "        train_ann = ann[ann[\"image\"].isin(train_images)]\n",
    "        val_ann = ann[ann[\"image\"].isin(val_images)]\n",
    "        test_ann = ann[ann[\"image\"].isin(test_images)]\n",
    "\n",
    "        # get datasets\n",
    "        self.train_dataset = Flickr8kDataSet(train_ann)\n",
    "        self.val_dataset = Flickr8kDataSet(val_ann)\n",
    "        self.test_dataset = Flickr8kDataSet(test_ann)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376b932",
   "metadata": {
    "papermill": {
     "duration": 0.008328,
     "end_time": "2024-01-05T17:31:16.932450",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.924122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Wir erstellen dieses DataModule und schauen uns ein Beispiel an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc5558",
   "metadata": {
    "papermill": {
     "duration": 0.113471,
     "end_time": "2024-01-05T17:31:17.054319",
     "exception": false,
     "start_time": "2024-01-05T17:31:16.940848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "flickr8k = Flickr8kDataModule()\n",
    "flickr8k.setup()\n",
    "\n",
    "images, captions, caption_tensor = flickr8k.train_dataset[0]\n",
    "plot_img(images, captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225702b",
   "metadata": {
    "papermill": {
     "duration": 0.009875,
     "end_time": "2024-01-05T17:31:17.074904",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.065029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Seems to be working :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c6911",
   "metadata": {
    "papermill": {
     "duration": 0.009791,
     "end_time": "2024-01-05T17:31:17.094489",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.084698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Schritt 2: Aufbau Modellierung \n",
    "Hier erstellen wir das Modell. Als Vorbereitung erstellen wir dafür einen `ImageEncoder`, welches die Bilder in numerische Features umwandelt, einen `CaptionEncoder`, welches die Tokens mit den gleichen Dimensionen Embedded und einen `CaptioningDecoder`, welcher das LSTM trainiert und neue Tokens vorhersagt.\n",
    "\n",
    "Der `ImageEncoder` ist so aufgebaut, dass verschiedene EfficinetNets, ResNets, VGGs und ViTs verwendet werden können. Anstatt den letzten Layer zu trainieren, wird der letzte Layer ersetzt und ein Dropout Layer mit nachfolgendem Linear Layer hinzugefügt. Somit kann Overfitting verhindert werden. Im Paper wird angegeben, dass Dropout das Modell noch besser gemacht hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3654b2",
   "metadata": {
    "papermill": {
     "duration": 0.016774,
     "end_time": "2024-01-05T17:31:17.121227",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.104453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self, cnn_model, embed_size, p_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        try:\n",
    "            # load the model\n",
    "            self.model = torchvision.models.get_model(cnn_model, weights=\"DEFAULT\")\n",
    "\n",
    "            # freeze all the layers\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # replace the classifier layer on efficientnet\n",
    "            if cnn_model.startswith(\"efficientnet\"):\n",
    "                self.model.classifier = nn.Sequential(\n",
    "                    nn.Dropout(p=p_dropout),\n",
    "                    nn.Linear(self.model.classifier[-1].in_features, embed_size),\n",
    "                )\n",
    "\n",
    "            # replace the fc layer on resnet\n",
    "            elif cnn_model.startswith(\"resnet\"):\n",
    "                self.model.fc = nn.Sequential(\n",
    "                    nn.Dropout(p=p_dropout),\n",
    "                    nn.Linear(self.model.fc.in_features, embed_size),\n",
    "                )\n",
    "\n",
    "            # replace the last classifier layer on vgg\n",
    "            elif cnn_model.startswith(\"vgg\"):\n",
    "                self.model.classifier[-1] = nn.Sequential(\n",
    "                    nn.Dropout(p=p_dropout),\n",
    "                    nn.Linear(self.model.classifier[-1].in_features, embed_size),\n",
    "                )\n",
    "\n",
    "            # replace the head layer on vit\n",
    "            elif cnn_model.startswith(\"vit\"):\n",
    "                self.model.heads.head = nn.Sequential(\n",
    "                    nn.Dropout(p=p_dropout),\n",
    "                    nn.Linear(self.model.heads.head.in_features, embed_size),\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Transfer Learning on Model {cnn_model} not implemented!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Cannot load model {cnn_model}!\") from e\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class CaptionEncoder(torch.nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        # initialize the nn.Embedding base class\n",
    "        super().__init__(vocab_size, embed_size)\n",
    "\n",
    "class CaptioningDecoder(torch.nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define the LSTM layer\n",
    "        self.lstm = torch.nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # define the output layer\n",
    "        self.linear = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # forward pass\n",
    "        output, _ = self.lstm(input)\n",
    "        output = self.linear(output)\n",
    "\n",
    "        # softmaxing\n",
    "        return torch.nn.functional.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fa842",
   "metadata": {
    "papermill": {
     "duration": 0.009903,
     "end_time": "2024-01-05T17:31:17.141286",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.131383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier testen wir, ob unsere Encoder und Decoder funktionieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49b270",
   "metadata": {
    "papermill": {
     "duration": 0.231871,
     "end_time": "2024-01-05T17:31:17.383004",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.151133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters for testing\n",
    "cnn_model = \"efficientnet_b0\" # list of models: torchvision.models.list_models()\n",
    "embed_size = 400\n",
    "p_dropout_cnn = 0.2\n",
    "hidden_size_lstm = 1000\n",
    "num_layers_lstm = 1\n",
    "\n",
    "# Test Image Encoder dimensions\n",
    "img_encoder = ImageEncoder(cnn_model, embed_size, p_dropout_cnn)\n",
    "img_encoder_out = img_encoder(images.unsqueeze(0))\n",
    "assert img_encoder(images.unsqueeze(0)).shape[1] == embed_size\n",
    "print(\"Image Encoder: OK\")\n",
    "print(f\"Image Encoder Output Shape: {img_encoder_out.shape}\\n\")\n",
    "\n",
    "# Test Caption Encoder dimensions\n",
    "cap_encoder = CaptionEncoder(VOCAB_SIZE, embed_size)\n",
    "cap_encoder_out = cap_encoder(caption_tensor)\n",
    "assert cap_encoder_out.shape[2] == embed_size\n",
    "print(\"Caption Encoder: OK\")\n",
    "print(f\"Caption Encoder Output Shape: {cap_encoder_out.shape}\\n\")\n",
    "\n",
    "# Print concatenation dimensions\n",
    "encoded_input = torch.cat((img_encoder_out.unsqueeze(1), cap_encoder_out[0,:,:].unsqueeze(0)), 1)\n",
    "print(f\"Encoded Input Shape: {encoded_input.shape}\\n\")\n",
    "\n",
    "# Test Caption Decoder dimensions\n",
    "cap_decoder = CaptioningDecoder(embed_size, hidden_size_lstm, VOCAB_SIZE, num_layers_lstm)\n",
    "cap_decoder_out = cap_decoder(encoded_input)\n",
    "assert cap_decoder_out.shape[2] == VOCAB_SIZE\n",
    "print(\"Caption Decoder: OK\")\n",
    "print(f\"Caption Decoder Output Shape: {cap_decoder_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b027de",
   "metadata": {
    "papermill": {
     "duration": 0.010312,
     "end_time": "2024-01-05T17:31:17.403978",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.393666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Seems to be working :)\n",
    "\n",
    "Die Encoder und Decoder werden im `ShowAndTell` Modell zusammengeführt. <br>\n",
    "Hier ist es wichtig anzumerken, dass die `forward` Methode des Modells nur im während dem Training verwendet wird, da beim Vorhersagen die Caption nicht bekannt ist und somit nicht mitgegeben werden kann. Um eine Caption aus nur einem Bild zu generieren, wird die `predict` Methode verwendet.<br>\n",
    "Auch noch Wissenswert: Bei der `forward` Methode wird automatisch Teacher Forcing verwendet. [Melchior, M. (2023, November 20). Teacher Forcing in PyTorch. Spaces.](https://spaces.technik.fhnw.ch/spaces/deep-learning/beitraege/teacher-forcing-in-pytorch#teacher-forcing-in-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0932d417",
   "metadata": {
    "papermill": {
     "duration": 0.025866,
     "end_time": "2024-01-05T17:31:17.439845",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.413979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShowAndTell(li.LightningModule):\n",
    "    def __init__(self, optimizer, lr, weight_decay, cnn_model, embed_size, p_dropout_cnn, vocab_size, hidden_size_lstm, num_layers_lstm, max_caption_length):\n",
    "        super().__init__()\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.cnn_model = cnn_model\n",
    "        self.embed_size = embed_size\n",
    "        self.p_dropout_cnn = p_dropout_cnn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size_lstm = hidden_size_lstm\n",
    "        self.num_layers_lstm = num_layers_lstm\n",
    "        self.max_caption_length = max_caption_length\n",
    "        \n",
    "        # initializing the submodules\n",
    "        self.img_encoder = ImageEncoder(self.cnn_model, self.embed_size, self.p_dropout_cnn)\n",
    "        self.caption_encoder = CaptionEncoder(self.vocab_size, self.embed_size)\n",
    "        self.captioning_decoder = CaptioningDecoder(self.embed_size, self.hidden_size_lstm, self.vocab_size, self.num_layers_lstm)\n",
    "        \n",
    "        # loss function \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        # metrics\n",
    "        self.bleu1_metric = BLEUScore(n_gram=1)\n",
    "        self.bleu2_metric = BLEUScore(n_gram=2)\n",
    "        self.bleu3_metric = BLEUScore(n_gram=3)\n",
    "        self.bleu4_metric = BLEUScore(n_gram=4)\n",
    "        \n",
    "        # scores and examples\n",
    "        self.scores = defaultdict(list)\n",
    "        self.examples = {\n",
    "            \"epoch\": [],\n",
    "            \"caption\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        # encode the images and captions\n",
    "        embedded_images = self.img_encoder(images).unsqueeze(1)\n",
    "\n",
    "        # iterate through the captions\n",
    "        decoded_captions = torch.zeros((captions.shape[1], captions.shape[0], captions.shape[2]+1, self.vocab_size), device=images.device)\n",
    "        for i, caption in enumerate(captions.permute(1, 0, 2)):\n",
    "            # encode the captions\n",
    "            embedded_captions = self.caption_encoder(caption)\n",
    "\n",
    "            # concatenate the images and captions\n",
    "            embedded_captions = torch.cat((embedded_images, embedded_captions), 1)\n",
    "\n",
    "            # decode the captions\n",
    "            decoded_caption = self.captioning_decoder(embedded_captions)\n",
    "            decoded_captions[i] = decoded_caption\n",
    "\n",
    "        # decode the captions\n",
    "        return decoded_captions\n",
    "    \n",
    "    def predict(self, images, max_caption_length=None):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # set the maximum caption length if not specified\n",
    "            if max_caption_length is None:\n",
    "                max_caption_length = self.max_caption_length\n",
    "\n",
    "            # encode the images\n",
    "            embedded_images = self.img_encoder(images).unsqueeze(1)\n",
    "\n",
    "            # initialize the caption with the start token\n",
    "            current_tokens = torch.tensor(tokens_str_to_int([START_TOKEN]), device=images.device)\n",
    "            current_tokens = current_tokens.repeat(len(images), 1)\n",
    "\n",
    "            for _ in range(max_caption_length-1):\n",
    "                # encode the current tokens\n",
    "                embedded_captions = self.caption_encoder(current_tokens)\n",
    "                embedded_captions = torch.cat((embedded_images, embedded_captions), 1)\n",
    "                \n",
    "                # decode the captions\n",
    "                output = self.captioning_decoder(embedded_captions).argmax(dim=-1)\n",
    "\n",
    "                # append the predicted token to the current tokens\n",
    "                current_tokens = torch.cat((current_tokens, output[:,-1:]), 1)\n",
    "\n",
    "            return current_tokens\n",
    "        \n",
    "    def _step(self, batch, batch_idx):\n",
    "        # forward pass\n",
    "        images, _, captions = batch\n",
    "        outputs = self(images, captions)[:, :, :-1, :] # remove last token, because it is not used as input\n",
    "        captions = captions.permute(1, 0, 2)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = self.loss_function(outputs.reshape(-1, VOCAB_SIZE), captions.reshape(-1))\n",
    "\n",
    "        # return loss\n",
    "        return loss\n",
    "    \n",
    "    def _step_return_results(self, batch, batch_idx):\n",
    "        # forward pass\n",
    "        images, captions, captions_int = batch\n",
    "        outputs = self(images, captions_int)[:, :, :-1, :] # remove last token, because it is not used as input\n",
    "        captions_int = captions_int.permute(1, 0, 2)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = self.loss_function(outputs.reshape(-1, VOCAB_SIZE), captions_int.reshape(-1))\n",
    "        \n",
    "        # forward pass w/o teacher forcing and wrangling for metrics\n",
    "        outputs = self.predict(images, MAX_CAPTION_LENGTH)\n",
    "        outputs = [\" \".join([token for token in tokens_int_to_str(caption) if token not in SPECIAL_TOKENS]) for caption in outputs.tolist()]\n",
    "        captions_ = [[[captions[j][i][k] for i in range(len(captions[0]))] for j in range(len(captions))] for k in range(len(captions[0][0]))]\n",
    "        captions_ = [[\" \".join([token for token in caption if token not in SPECIAL_TOKENS]) for caption in captions] for captions in captions_]\n",
    "        \n",
    "        # saving into dict\n",
    "        predictions = defaultdict(list)\n",
    "        predictions[\"images\"] = images\n",
    "        predictions[\"captions\"] = captions_\n",
    "        predictions[\"outputs\"] = outputs\n",
    "\n",
    "        # return loss and predictions\n",
    "        return loss, predictions\n",
    "            \n",
    "    def training_step(self, batch, _):\n",
    "        loss = self._step(batch, _)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, predictions = self._step_return_results(batch, _)\n",
    "\n",
    "        bleu1 = self.bleu1_metric(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "        bleu2 = self.bleu2_metric(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "        bleu3 = self.bleu3_metric(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "        bleu4 = self.bleu4_metric(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "\n",
    "        self.scores['val_loss'].append(loss)\n",
    "        self.scores['val_BLEU1'].append(bleu1)\n",
    "        self.scores['val_BLEU2'].append(bleu2)\n",
    "        self.scores['val_BLEU3'].append(bleu3)\n",
    "        self.scores['val_BLEU4'].append(bleu4)\n",
    "        \n",
    "        # log first example of batch\n",
    "        self.examples[\"epoch\"].append(self.current_epoch)\n",
    "        self.examples[\"caption\"].append(predictions[\"captions\"][0][0])\n",
    "        self.examples[\"prediction\"].append(predictions[\"outputs\"][0])\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log('val_loss',  torch.tensor(self.scores['val_loss']).mean())\n",
    "        self.log('val_BLEU1', torch.tensor(self.scores['val_BLEU1']).mean())\n",
    "        self.log('val_BLEU2', torch.tensor(self.scores['val_BLEU2']).mean())\n",
    "        self.log('val_BLEU3', torch.tensor(self.scores['val_BLEU3']).mean())\n",
    "        self.log('val_BLEU4', torch.tensor(self.scores['val_BLEU4']).mean())\n",
    "        self.logger.log_text(key=\"val_samples\", dataframe=pd.DataFrame.from_dict(self.examples))\n",
    "        \n",
    "        self.scores = defaultdict(list)\n",
    "\n",
    "    def test_step(self, batch, _):\n",
    "        loss, predictions = self._step_return_results(batch, _)\n",
    "            \n",
    "        bleu1 = self.bleu1_metric(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "        bleu2 = self.bleu2_metric(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "        bleu3 = self.bleu3_metric(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "        bleu4 = self.bleu4_metric(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "\n",
    "        self.scores['test_loss'].append(loss)\n",
    "        self.scores['test_BLEU1'].append(bleu1)\n",
    "        self.scores['test_BLEU2'].append(bleu2)\n",
    "        self.scores['test_BLEU3'].append(bleu3)\n",
    "        self.scores['test_BLEU4'].append(bleu4)\n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        self.log('test_loss',  torch.tensor(self.scores['test_loss']).mean())\n",
    "        self.log('test_BLEU1', torch.tensor(self.scores['test_BLEU1']).mean())\n",
    "        self.log('test_BLEU2', torch.tensor(self.scores['test_BLEU2']).mean())\n",
    "        self.log('test_BLEU3', torch.tensor(self.scores['test_BLEU3']).mean())\n",
    "        self.log('test_BLEU4', torch.tensor(self.scores['test_BLEU4']).mean())\n",
    "               \n",
    "        self.scores = defaultdict(list)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # define the optimizer\n",
    "        match self.optimizer:\n",
    "            case \"SGD\":\n",
    "                return torch.optim.SGD(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "            case \"Adam\":\n",
    "                return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay) \n",
    "            case \"AdamW\":\n",
    "                return torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "            case _:\n",
    "                raise ValueError(f\"Optimizer {self.optimizer} not supported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005adb0e",
   "metadata": {
    "papermill": {
     "duration": 0.009927,
     "end_time": "2024-01-05T17:31:17.459763",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.449836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hier machen wir einen Testlauf mit dem Modell, um zu schauen, ob es funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b857ba7f",
   "metadata": {
    "papermill": {
     "duration": 4.422954,
     "end_time": "2024-01-05T17:31:21.892633",
     "exception": false,
     "start_time": "2024-01-05T17:31:17.469679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ShowAndTell(\n",
    "    optimizer=\"Adam\",\n",
    "    lr=0.0001,\n",
    "    weight_decay=0.001,\n",
    "    cnn_model=\"efficientnet_b0\",\n",
    "    embed_size=512,\n",
    "    p_dropout_cnn=0.5,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_size_lstm=1000,\n",
    "    num_layers_lstm=1,\n",
    "    max_caption_length=MAX_CAPTION_LENGTH,\n",
    ")\n",
    "\n",
    "trainer = li.Trainer(\n",
    "    max_epochs=1,\n",
    "    log_every_n_steps=1,\n",
    "    fast_dev_run=2,\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model, flickr8k.train_dataloader(), flickr8k.val_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22647cc",
   "metadata": {
    "papermill": {
     "duration": 0.01026,
     "end_time": "2024-01-05T17:31:21.913711",
     "exception": false,
     "start_time": "2024-01-05T17:31:21.903451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Das Modell hat beim Testlauf keinen Fehler geworfen. Seems to be working :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4538a4",
   "metadata": {
    "papermill": {
     "duration": 0.010275,
     "end_time": "2024-01-05T17:31:21.934331",
     "exception": false,
     "start_time": "2024-01-05T17:31:21.924056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Schritt 3: Training\n",
    "\n",
    "Ich optimiere mein Modell jetzt auf folgende Hyperparameter:\n",
    "- Optimizer: `Adam`\n",
    "  - SGD wurde zwar im Paper verwendet, gab bei meinen Tests jedoch suboptimale Resultate.\n",
    "- Lernrate (lr): `0.001` oder `0.0003`\n",
    "  - Die Lernrate wurde im Paper nicht erwähnt. Da Adam verwendet wird, habe ich kleine Lernraten gewählt.\n",
    "- L2 Regularisierung (weight_decay): `0.00001`\n",
    "  - Die L2 Regularisierung wurde im Paper nicht erwähnt. Da ich aber beim CNN bereits Dropout verwende, wähle ich hier einen kleinen Wert.\n",
    "- CNN Modell: `efficientnet_b0` oder `vgg11` oder `resnet18`\n",
    "  - Ich will verschiedene Modelle testen, um zu sehen, welches am besten funktioniert. Ich nehme hier die kleinsten Modelle, um keinen zu grossen Rechenaufwand beim Grid Search zu haben. Danach kann ich die grössere Version dieser Architekturen testen.\n",
    "- Embedding Grösse (embed_size): `512`\n",
    "  - Diese Embedding Grösse wurde im Paper verwendet, deswegen verwende ich sie auch.\n",
    "- Dropout Wahrscheinlichkeit (p_dropout_cnn): `0.2` oder `0.5`\n",
    "  - Hier will ich schwache und starke Regularisierung und deren Auswirkung auf die Generalisierbarkeit des Modells testen.\n",
    "- LSTM Memory (hidden_size_lstm): `512`\n",
    "  - Diese Hidden Size wurde im Paper verwendet, deswegen verwende ich sie auch.\n",
    "- Anzahl der LSTM Layer (num_layers_lstm): `1` oder `2`\n",
    "  - Im Paper wurden hier keine spezifische Anzahl Layer erwähnt. Hier teste ich den Unterschied zwischen einem und zwei Layer.\n",
    "- Anzahl Epochen (n_epochs): `30`\n",
    "  - Hier wähle ich eine etwas grosse Anzahl Epochen, da ich in vorherigen Tests gesehen habe, dass es bis hier am meisten lernt.\n",
    "\n",
    "Insgesamt haben wir hier 24 verschiedene Modelle, welche wir trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6cbb8",
   "metadata": {
    "papermill": {
     "duration": 0.015822,
     "end_time": "2024-01-05T17:31:21.960419",
     "exception": false,
     "start_time": "2024-01-05T17:31:21.944597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"name\": \"ShowAndTell\",\n",
    "    \"parameters\": {\n",
    "        \"optimizer\": {\"values\": [\"Adam\"]},\n",
    "        \"lr\": {\"values\": [0.001, 0.0003]},\n",
    "        \"weight_decay\": {\"values\": [0.00001]},\n",
    "        \"cnn_model\": {\"values\": [\"efficientnet_b0\", \"vgg11\", \"resnet18\"]},\n",
    "        \"embed_size\": {\"values\": [512]},\n",
    "        \"p_dropout_cnn\": {\"values\": [0.5, 0.2]},\n",
    "        \"hidden_size_lstm\": {\"values\": [512]},\n",
    "        \"num_layers_lstm\": {\"values\": [1, 2]},\n",
    "        \"n_epochs\": {\"values\": [30]},\n",
    "    },\n",
    "}\n",
    "\n",
    "def train():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "        wandb_logger = li.loggers.WandbLogger(log_model=True)\n",
    "\n",
    "        model = ShowAndTell(\n",
    "            optimizer=config.optimizer,\n",
    "            lr=config.lr,\n",
    "            weight_decay=config.weight_decay,\n",
    "            cnn_model=config.cnn_model,\n",
    "            embed_size=config.embed_size,\n",
    "            p_dropout_cnn=config.p_dropout_cnn,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size_lstm=config.hidden_size_lstm,\n",
    "            num_layers_lstm=config.num_layers_lstm,\n",
    "            max_caption_length=MAX_CAPTION_LENGTH,\n",
    "        )\n",
    "\n",
    "        wandb_logger.watch(model)\n",
    "        trainer = li.Trainer(\n",
    "            max_epochs=config.n_epochs,\n",
    "            log_every_n_steps=1,\n",
    "            logger=wandb_logger,\n",
    "            fast_dev_run=False,\n",
    "            enable_progress_bar=False,\n",
    "            enable_model_summary=False,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, flickr8k.train_dataloader(), flickr8k.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e3909",
   "metadata": {
    "papermill": {
     "duration": 16981.801876,
     "end_time": "2024-01-05T22:14:23.772572",
     "exception": false,
     "start_time": "2024-01-05T17:31:21.970696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sweep_id = wandb.sweep(sweep_config, project=\"fhnw-del-mc2\")\n",
    "#wandb.agent(sweep_id, train)\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846accf7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Sweep Ergebnisse:\n",
    "https://api.wandb.ai/links/gabrieltorresgamez/a9hpsrul\n",
    "\n",
    "<iframe src=\"https://wandb.ai/gabrieltorresgamez/fhnw-del-mc2/reports/Sweep1--Vmlldzo2MTk0MDgw\" style=\"border:none;height:500px;width:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaea175",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Bei diesem Sweep konnte ich folgende Kenntnisse gewinnen:\n",
    "- Mit einer Learning Rate von `0.001` fing das Modell an zu Overfitten. Nach wenigen Epochen wurde aber bereits ein sehr tiefer Wert erreicht. Hier muss entweder eine kleinere Lernrate gewählt werden und länger trainiert oder ein früheres Stoppen des Trainings (Early Stopping) werden.\n",
    "- Beim LSTM gaben Modelle mit `2` Layer schlechtere Resultate als der Modelle mit nur `1` einem Layer. Es sieht so aus, als würde ein zweites Layer zu Overfitting führen.\n",
    "- Bei der Wahl des CNNs haben alle Modelle in etwa gleich stark performed. Die Differenz zwischen den verschiedenen Modellen war nicht signifikant.\n",
    "- Die Differenz zwischen einer Dropout Wahrscheinlichkeit von `0.2` und `0.5` war auch nicht signifikant.\n",
    "\n",
    "Deswegen werde ich für die nächsten Tests folgende Hyperparameter anzupassen:\n",
    "- Lernrate (lr): `0.001 mit EarlyStopping`\n",
    "- LSTM Layer (num_layers_lstm): `1`\n",
    "- CNN Model (cnn_model): `ResNet18 oder grösser`\n",
    "- Dropout Wahrscheinlichkeit (p_dropout_cnn): `0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f4445",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hier passe ich die neuen Hyperparameter an.\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"name\": \"ShowAndTell2\",\n",
    "    \"parameters\": {\n",
    "        \"optimizer\": {\"values\": [\"Adam\"]},\n",
    "        \"lr\": {\"values\": [0.001]},\n",
    "        \"weight_decay\": {\"values\": [0.00001]},\n",
    "        \"cnn_model\": {\"values\": [\"resnet18\", \"resnet152\"]},\n",
    "        \"embed_size\": {\"values\": [512]},\n",
    "        \"p_dropout_cnn\": {\"values\": [0.5]},\n",
    "        \"hidden_size_lstm\": {\"values\": [512]},\n",
    "        \"num_layers_lstm\": {\"values\": [1]},\n",
    "        \"n_epochs\": {\"values\": [30]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Hier füge ich beim Training das Early Stopping hinzu.\n",
    "def train():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "        wandb_logger = li.loggers.WandbLogger(log_model=True)\n",
    "\n",
    "        model = ShowAndTell(\n",
    "            optimizer=config.optimizer,\n",
    "            lr=config.lr,\n",
    "            weight_decay=config.weight_decay,\n",
    "            cnn_model=config.cnn_model,\n",
    "            embed_size=config.embed_size,\n",
    "            p_dropout_cnn=config.p_dropout_cnn,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size_lstm=config.hidden_size_lstm,\n",
    "            num_layers_lstm=config.num_layers_lstm,\n",
    "            max_caption_length=MAX_CAPTION_LENGTH,\n",
    "        )\n",
    "\n",
    "        wandb_logger.watch(model)\n",
    "        trainer = li.Trainer(\n",
    "            max_epochs=config.n_epochs,\n",
    "            log_every_n_steps=1,\n",
    "            logger=wandb_logger,\n",
    "            fast_dev_run=False,\n",
    "            enable_progress_bar=False,\n",
    "            enable_model_summary=False,\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=0)]\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, flickr8k.train_dataloader(), flickr8k.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de4fe5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"fhnw-del-mc2\")\n",
    "wandb.agent(sweep_id, train)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534208ee",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Sweep Ergebnisse:\n",
    "https://api.wandb.ai/links/gabrieltorresgamez/x9sixaij\n",
    "\n",
    "<iframe src=\"https://wandb.ai/gabrieltorresgamez/fhnw-del-mc2/reports/Sweep2--Vmlldzo2MTk0MTQx\" style=\"border:none;height:500px;width:100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4e76f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Bei diesem Sweep konnte ich folgende Kenntnisse gewinnen:\n",
    "- Durch das wenige Rauschen im Validierungsloss funktioniert Early Stopping gut.\n",
    "- Ein grösseres CNN als Image Encoder führt zu einem besseren Validierungsloss des Captioning Modells, hat aber als Drawback die höhere Inference Zeit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a86721",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Schritt 4: Evaluation\n",
    "\n",
    "In diesem Abschnitt evaluiere ich das beste Modell (tiefster Validierungsloss) vom Training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e711c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Hier laden wir von wandb das beste Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7d8b8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download model as wandb artifact\n",
    "model_artifact = wandb.Api().artifact('gabrieltorresgamez/fhnw-del-mc2/model-j5l55qmg:v0', type='model')\n",
    "model_path = model_artifact.file('./artifacts/best_model')\n",
    "\n",
    "# Load the weights into the model\n",
    "model = ShowAndTell.load_from_checkpoint(\n",
    "    model_path, \n",
    "    optimizer=\"Adam\", \n",
    "    lr=0.001, \n",
    "    weight_decay=0.00001, \n",
    "    cnn_model=\"resnet152\", \n",
    "    embed_size=512, \n",
    "    p_dropout_cnn=0.5, \n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    hidden_size_lstm=512, \n",
    "    num_layers_lstm=1, \n",
    "    max_caption_length=MAX_CAPTION_LENGTH,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28ff2d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Danach lassen wir alle Bilder durch das Modell laufen, generieren die Captions und speichern alles ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684e5e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(DEVICE).eval()\n",
    "\n",
    "# Predict every image in the test set and save the predictions\n",
    "predictions = defaultdict(list)\n",
    "for batch in tqdm(iter(flickr8k.test_dataloader())):\n",
    "    images, captions, captions_int = batch\n",
    "    images, caption_int = images.to(DEVICE), captions_int.to(DEVICE)\n",
    "    \n",
    "    # forward pass w/o teacher forcing and wrangling for metrics\n",
    "    outputs = self.predict(images, MAX_CAPTION_LENGTH)\n",
    "    outputs = [\" \".join([token for token in tokens_int_to_str(caption) if token not in SPECIAL_TOKENS]) for caption in outputs.tolist()]\n",
    "    captions_ = [[[captions[j][i][k] for i in range(len(captions[0]))] for j in range(len(captions))] for k in range(len(captions[0][0]))]\n",
    "    captions_ = [[\" \".join([token for token in caption if token not in SPECIAL_TOKENS]) for caption in captions] for captions in captions_]\n",
    "        \n",
    "    # saving\n",
    "    predictions[\"images\"].append(images)\n",
    "    predictions[\"captions\"].append(captions_)\n",
    "    predictions[\"outputs\"].append(outputs)\n",
    "\n",
    "# Concatenate the predictions\n",
    "predictions[\"images\"] = torch.cat(predictions[\"images\"])\n",
    "predictions[\"captions\"] = sum(predictions[\"captions\"], [])\n",
    "predictions[\"outputs\"] = sum(predictions[\"outputs\"], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab58567",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Hier berechnen wir alle Metriken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e117e-69cc-456f-ab5e-583c14fed1f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bleu1 = BLEUScore(n_gram=1)(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "bleu2 = BLEUScore(n_gram=2)(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "bleu3 = BLEUScore(n_gram=3)(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "bleu4 = BLEUScore(n_gram=4)(predictions[\"outputs\"], predictions[\"captions\"])\n",
    "\n",
    "print(f\"BLEU-1:\\t\\t{bleu1:.4f}\")\n",
    "print(f\"BLEU-2:\\t\\t{bleu2:.4f}\")\n",
    "print(f\"BLEU-3:\\t\\t{bleu3:.4f}\")\n",
    "print(f\"BLEU-4:\\t\\t{bleu4:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e1c82",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Hier erkennen wir, dass der BLEU-1 Scores doch viel besser sind als beim Training des ersten ShowAndTell Modell. Dies deutet darauf hin, dass die Implementation des BLEU-1 Scores inkorrekt war. Man sollte also immer die Scores mit anderen Modellen oder dem Paper vergleichen, um zu sehen, ob die Scores Sinn machen.\n",
    "\n",
    "Im Paper wird für den Flickr8k Datensatz ein BLEU-1 Score von 63 (`0.63`) gegeben. Wir haben hier einen etwas tieferen Score von ~57 (`0.5666`) erhalten, was darauf hindeutet, dass unser Modell fast so gut wie das Modell vom Paper performt aber es noch Verbesserungspotenzial gibt. Das ist erstaunlich, vorallem weil wir nur 17 Epochen (9min auf einer NVIDIA RTX A4500) trainiert haben.\n",
    "\n",
    "Jedoch könnten wir noch mehr probieren, um die Performance zu verbessern:\n",
    "- `Tiefere Lernrate und länger trainieren`\n",
    "- `Andere Optimizer probieren`\n",
    "- `Anders Tokenisieren`\n",
    "- `Grössere Embedding Size`\n",
    "- `Vortrainierte Embeddings nutzen`\n",
    "- `Grossschreibung wegnehmen`\n",
    "- `Beim LSTM eine grössere Hidden Size probieren`\n",
    "- `Mehr Regularisierung einbauen und länger trainieren`\n",
    "- `Beam Search implementieren`\n",
    "- `Weitere CNNs wie z.B. ViT probieren`\n",
    "- `Augmentierung der Bilder hinzufügen`\n",
    "- `Attention im CNN implementieren (ShowAttendAndTell)`\n",
    "- `Mehr Daten nutzen (z.B. Flickr30k)`\n",
    "- `LSTM mit anderen Architekturen austauschen (z.B. GRU, Transformer)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd08be5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Zum Abschluss schauen wir uns noch ein paar Beispiele an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3ca0e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plot_img(predictions[\"images\"][i*5].cpu(), predictions[\"captions\"][i*5], stringify=False)\n",
    "    print(f\"\\nPrediction: {predictions['outputs'][i*5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895c6aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Bei den Beispielen im Testdatensatz sehen wir, dass unsere Modell auf einige Bilder gut und auf einige eher schlechter funktioniert. <br>\n",
    "Es erkennt meistens bestimmte Muster im Bild gut, macht jedoch bei Details einige Fehler. <br>\n",
    "Das Verhalten ist bekannt und wird auch im Paper erwähnt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1790c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275578c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Schritt 5: Präsentation / Bericht \n",
    "- Präsentation (~10m): Kurze Präsentation mit Diskussion der wichtigsten Ergebnisse. <br> \n",
    "- Q&A (~10min): Klärung von Verständnisfragen zum Paper und der Umsetzung. <br>\n",
    "- Bericht in Form eines gut dokumentierten, übersichtlichen Jupyter Notebooks. \n",
    "\n",
    "Dieses soll schliesslich auch abgegeben werden und dem Fachexperten erlauben, die <br>\n",
    "Schritte nachzuvollziehen (allenfalls auch das Training erneut laufen zu lassen). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ce772",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Beurteilung \n",
    "Beurteilt wird auf Basis des abgegebenen Notebooks:  \n",
    "- Vollständige und korrekte Umsetzung der vereinbarten Aufgabestellung. \n",
    "- Klare, gut-strukturierte Umsetzung.  \n",
    "- Schlüssige Beschreibung und Interpretation der Ergebnisse. Gut gewählte und gut <br> kommentierten Plots und Tabellen. \n",
    "- Vernünftiger Umgang mit (Computing-)Ressourcen. \n",
    "- Verständliche Präsentation der Ergebnisse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d7407",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Referenzen, Key Words \n",
    "- Word Embedding (z.B. word2vec, glove), um Wörter in numerische Vektoren in einem <br> geeignet dimensionierten Raum zu mappen. Siehe z.B. Andrew Ng, Coursera: <br>https://www.coursera.org/lecture/nlp-sequence-models/learning-word-embeddings-APM5s  \n",
    "- Bild Embedding mittels vortrainierten (evt. retrained) Netzwerken wie beispielsweise <br>ResNet, GoogLeNet, EfficientNet oder ähnlich. Transfer-Learning. \n",
    "- Seq2Seq Models bekannt für Sprach-Übersetzung.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16995.420265,
   "end_time": "2024-01-05T22:14:25.726919",
   "environment_variables": {},
   "exception": true,
   "input_path": "fhnw-del-mc2.ipynb",
   "output_path": "fhnw-del-mc2.ipynb",
   "parameters": {},
   "start_time": "2024-01-05T17:31:10.306654",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "049ab7b13f5f4319ab222686be9d05f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": "100%"
      }
     },
     "336771e00c9f40fd83734d0d670c713b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9408a391feb5416aa2e658a1faeda632",
       "placeholder": "​",
       "style": "IPY_MODEL_9487c6b26c6d44329d50841239d99037",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:00&lt;00:00,  2.08it/s]"
      }
     },
     "352b3df35a5f4052b8da4f26ceea2fe8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "395d18b7b62b489fb8fcb6db644f7bdc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "582bdf80b46341508f43fafa0c807230": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_643299c614164675a5d1ecff24ef95c9",
       "placeholder": "​",
       "style": "IPY_MODEL_bfa751a483cf459c964fb521e8c7b2d9",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:03&lt;00:00,  0.56it/s]"
      }
     },
     "643299c614164675a5d1ecff24ef95c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76c22981de7749e0a4ff30bcc8b827a0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7aa38106475141e985fd9d1b62dfd310": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "93183e2d633141f2938d34d357400cb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e6aaf64505eb44a7bc714e642d1c6877",
        "IPY_MODEL_a65f648284cc41028f2ef695a0c97d8c",
        "IPY_MODEL_336771e00c9f40fd83734d0d670c713b"
       ],
       "layout": "IPY_MODEL_049ab7b13f5f4319ab222686be9d05f1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "9408a391feb5416aa2e658a1faeda632": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9487c6b26c6d44329d50841239d99037": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "97663f358071425b9ae8c01de3bf3594": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9a7703d014d6424fb6073fa1f31f908a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a65f648284cc41028f2ef695a0c97d8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_395d18b7b62b489fb8fcb6db644f7bdc",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_97663f358071425b9ae8c01de3bf3594",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "ba44ac8d99934f62a2ba004493356163": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc26b600cb0d461fbdbf8e9d22409ef7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bfa751a483cf459c964fb521e8c7b2d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2f511d5aa3d4115a9f56b48e9066bb1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6aaf64505eb44a7bc714e642d1c6877": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_76c22981de7749e0a4ff30bcc8b827a0",
       "placeholder": "​",
       "style": "IPY_MODEL_7aa38106475141e985fd9d1b62dfd310",
       "tabbable": null,
       "tooltip": null,
       "value": "Validation DataLoader 0: 100%"
      }
     },
     "ec329f54e6b74f03b106243259570ca0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ba44ac8d99934f62a2ba004493356163",
       "placeholder": "​",
       "style": "IPY_MODEL_bc26b600cb0d461fbdbf8e9d22409ef7",
       "tabbable": null,
       "tooltip": null,
       "value": "Epoch 0: 100%"
      }
     },
     "f43ab3e0e5704ccb957d83b87fed8d87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ec329f54e6b74f03b106243259570ca0",
        "IPY_MODEL_f97392d194ad43f7866585269cfab253",
        "IPY_MODEL_582bdf80b46341508f43fafa0c807230"
       ],
       "layout": "IPY_MODEL_352b3df35a5f4052b8da4f26ceea2fe8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f97392d194ad43f7866585269cfab253": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d2f511d5aa3d4115a9f56b48e9066bb1",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9a7703d014d6424fb6073fa1f31f908a",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
