{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 30%; float: right; margin: 10px; margin-right: 5%;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/FHNW_Logo.svg/2560px-FHNW_Logo.svg.png\" width=\"500\" style=\"float: left; filter: invert(50%);\"/>\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align: left; margin-top: 10px; float: left; width: 60%;\">\n",
    "    Mini-Challenge 2:<br>Paper-Studium und Umsetzung\n",
    "</h1>\n",
    "\n",
    "<p style=\"clear: both; text-align: left;\">\n",
    "    Bearbeitet durch Gabriel Torres Gamez im FS 2023.\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ziel\n",
    "Vertiefung in ein eher aktuelles Paper aus der Forschung und Umsetzung eines darin <br>\n",
    "beschriebenen oder verwandten Tasks - gemäss Vereinbarung mit dem Fachcoach.  \n",
    "\n",
    "Beispiel: Implementiere, trainiere und validiere ein Deep Learning Modell für Image <br>\n",
    "Captioning wie beschrieben im Paper Show and Tell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Hier wird vor der Bearbeitung alles aufgesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "print(\"Installing requirements...\")\n",
    "%pip install --upgrade pip -q\n",
    "%pip install --upgrade -r requirements.txt -q\n",
    "print(\"Done installing requirements!\\n\")\n",
    "\n",
    "# Standard Libraries\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# 3rd Party Libraries\n",
    "import dvclive\n",
    "import gensim\n",
    "import matplotlib as mpl\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pytorch_lightning as li\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm as tqdm_module\n",
    "import wandb\n",
    "\n",
    "\n",
    "## Specific Imports\n",
    "from argparse import Namespace\n",
    "from dvclive.lightning import DVCLiveLogger\n",
    "from gensim.models import Word2Vec\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize as nltk_word_tokenize\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Print information about the environment\n",
    "print(\"Packages: \")\n",
    "print(f\" | Python Version: {sys.version}\")\n",
    "#print(f\" | Dvclive Version: {dvclive.__version__}\")\n",
    "print(f\" | Gensim Version: {gensim.__version__}\")\n",
    "print(f\" | Matplotlib Version: {mpl.__version__}\")\n",
    "print(f\" | NLTK Version: {nltk.__version__}\")\n",
    "print(f\" | Numpy Version: {np.__version__}\")\n",
    "print(f\" | PyTorch Lightning Version: {li.__version__}\")\n",
    "print(f\" | PyTorch Version: {torch.__version__}\")\n",
    "print(f\" | Torchvision Version: {torchvision.__version__}\")\n",
    "print(f\" | tqdm Version: {tqdm_module.__version__}\")\n",
    "print(f\" | Wandb Version: {wandb.__version__}\")\n",
    "print()\n",
    "\n",
    "print(\"Backends: \")\n",
    "print(f\" | CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"  | CUDA Built: {torch.backends.cuda.is_built()}\")\n",
    "print(f\" | MPS: {torch.backends.mps.is_available()}\")\n",
    "print(f\"  | MPS Built: {torch.backends.mps.is_built()}\")\n",
    "print()\n",
    "\n",
    "print(\"Ressources: \")\n",
    "print(f\" | CPU Threads: {torch.get_num_threads()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\" | GPU {i+1}: {torch.cuda.get_device_name(i)}\\n\")\n",
    "\n",
    "# Set torch settings\n",
    "if torch.cuda.is_available():  # explicitly use NVIDIA GPU's\n",
    "    accelerator = \"cuda\"\n",
    "elif torch.backends.mps.is_available():  # use APPLE Metal\n",
    "    accelerator = \"mps\"\n",
    "else:  # CPU\n",
    "    accelerator = \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# silent warnings\n",
    "%env WANDB_SILENT=True\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "\n",
    "# download the nltk tokenizer\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Final message\n",
    "print(\"Setup complete!\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvector_size = 1000\n",
    "t2v_epochs = 300\n",
    "num_workers = 8\n",
    "batch_size = 64\n",
    "\n",
    "# NIC\n",
    "epochs_train = 100\n",
    "hidden_size_lstm = 1000\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsschritte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: Daten \n",
    "Gemäss Vereinbarung (für Captioning: Flickr8k-Daten). \n",
    "\n",
    "Absprache/Beschluss mit Coach und Beschluss, was evaluiert werden soll. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    return [\"<start>\"] + nltk_word_tokenize(text) + [\"<end>\"]\n",
    "\n",
    "\n",
    "def load_captions(ann_file, tokenizer):\n",
    "    anns = open(ann_file, \"r\", encoding=\"utf-8\").read().split(\"\\n\")\n",
    "\n",
    "    # tokenize captions\n",
    "    captions = {}\n",
    "    for line in anns:\n",
    "        if len(line):\n",
    "            file, caption = line.split(\";\")\n",
    "            if file not in captions:\n",
    "                captions[file] = [tokenizer(caption.lower())]\n",
    "            else:\n",
    "                captions[file].append(tokenizer(caption.lower()))\n",
    "\n",
    "    # pad captions\n",
    "    max_len = max(len(caption) for file in captions for caption in captions[file])\n",
    "    for file in captions:\n",
    "        for caption in captions[file]:\n",
    "            caption += [\"<pad>\"] * (max_len - len(caption))\n",
    "\n",
    "    return captions\n",
    "\n",
    "\n",
    "def load_images(root, captions):\n",
    "    images = {}\n",
    "    for file in tqdm(captions, desc=\"Loading images\"):\n",
    "        image = Image.open(f\"{root}/{file}\").convert(\"RGB\")\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        image = transform(image)\n",
    "        images[file] = image\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"data/images\"\n",
    "ann_file = \"data/captions.txt\"\n",
    "\n",
    "captions_dict = load_captions(ann_file, word_tokenize)\n",
    "images_dict = load_images(root, captions_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random picture and its captions\n",
    "file, caption = list(captions_dict.items())[1]\n",
    "image = images_dict[file]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(f\"Filename: {file}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Captions:\")\n",
    "for caption_ in caption:\n",
    "    print(f\" | {caption_}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding via EfficientNetB0\n",
    "def get_model_efficientnetb0():\n",
    "    # Load the model\n",
    "    model = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "\n",
    "    # Transfer the model to the appropriate device\n",
    "    model.to(accelerator)\n",
    "\n",
    "    # Remove the last layer\n",
    "    model._fc = torch.nn.Identity()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def encode_images(images, model, batch_size=batch_size):\n",
    "    # Dictionary to store the encoded images\n",
    "    encoded_images = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the images in batches\n",
    "        for i in tqdm(\n",
    "            range(0, len(images), batch_size),\n",
    "            desc=f\"Encoding images in batches of {batch_size}\",\n",
    "        ):\n",
    "            # Get the file names for the current batch\n",
    "            batch_files = list(images.keys())[i : i + batch_size]\n",
    "            # Get the images for the current batch and move them to the appropriate device\n",
    "            batch_images = torch.stack(list(images.values())[i : i + batch_size]).to(\n",
    "                accelerator\n",
    "            )\n",
    "            # Encode the images in the batch and move them to the CPU\n",
    "            encoded_batch = model(batch_images).cpu()\n",
    "\n",
    "            # Store the encoded images in the dictionary\n",
    "            for j, file in enumerate(batch_files):\n",
    "                encoded_images[file] = encoded_batch[j]\n",
    "\n",
    "    return encoded_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetb0 = get_model_efficientnetb0()\n",
    "encoded_images = encode_images(images_dict, efficientnetb0, batch_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the captions\n",
    "The Word2Vec model is trained before the Train, Validation and Test Split because we want to use the same word embedding for all the splits. \n",
    "\n",
    "This could falsify the results because the model could learn words that are in the validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vec(captions):\n",
    "    # All captions as a vector\n",
    "    captions_vector = [\n",
    "        image for captions in captions_dict.values() for image in captions\n",
    "    ]\n",
    "\n",
    "    # Create the model\n",
    "    model = Word2Vec(\n",
    "        captions_vector,\n",
    "        vector_size=wordvector_size,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=num_workers,\n",
    "        sg=1,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.train(captions, total_examples=len(captions), epochs=t2v_epochs)\n",
    "\n",
    "    # Encode the captions\n",
    "    encoded_captions = {}\n",
    "    for file in tqdm(captions, desc=\"Recreating captions as vectors\"):\n",
    "        for i, caption in enumerate(captions[file]):\n",
    "            if file not in encoded_captions:\n",
    "                encoded_captions[file] = [model.wv[caption]]\n",
    "            else:\n",
    "                encoded_captions[file].append(model.wv[caption])\n",
    "\n",
    "    return encoded_captions, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_captions, word2vec = word_to_vec(captions_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(encoded_captions)\n",
    "\n",
    "# Splitting into 70% train, 20% validation and 10% test data\n",
    "train_keys, val_keys, test_keys = random_split(\n",
    "    list(encoded_captions.keys()),\n",
    "    (0.7, 0.2, 0.1),\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "# Splitting into train, validation and test data\n",
    "train_captions = {key: encoded_captions[key] for key in train_keys}\n",
    "val_captions = {key: encoded_captions[key] for key in val_keys}\n",
    "test_captions = {key: encoded_captions[key] for key in test_keys}\n",
    "\n",
    "train_images = {key: encoded_images[key] for key in train_keys}\n",
    "val_images = {key: encoded_images[key] for key in val_keys}\n",
    "test_images = {key: encoded_images[key] for key in test_keys}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NICDataset(Dataset):\n",
    "    def __init__(self, encoded_images, encoded_captions, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.encoded_images = encoded_images\n",
    "        self.encoded_captions = [\n",
    "            {file: caption}\n",
    "            for file in encoded_captions\n",
    "            for caption in encoded_captions[file]\n",
    "        ]\n",
    "        assert len(self.encoded_captions) / 5 == len(\n",
    "            self.encoded_images\n",
    "        ), \"For every image there should be 5 captions\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file, caption = list(self.encoded_captions[index].items())[0]\n",
    "        caption = torch.tensor(caption).long()\n",
    "        image = self.encoded_images[file]\n",
    "\n",
    "        return file, image, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NICDataset(train_images, train_captions, word2vec)\n",
    "val_dataset = NICDataset(val_images, val_captions, word2vec)\n",
    "test_dataset = NICDataset(test_images, test_captions, word2vec)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=len(val_dataset),\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(test_dataset),\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2: Aufbau Modellierung \n",
    "Überlege Dir, welche Modell-Architektur Sinn machen könnte. Das Paper von Vinyals <br>\n",
    "(https://arxiv.org/pdf/1411.4555.pdf) war wichtig in diesem Zusammenhang. du kannst <br>\n",
    "Dich auf Ihre Architektur beziehen oder auch eine eigene, evt. einfachere verwenden.  <br>\n",
    "Zwei Modell-Varianten sollen aufgebaut werden: \n",
    " \n",
    "Absprache/Beschluss mit Coach und Beschluss, was evaluiert werden soll. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIC(li.LightningModule):\n",
    "    def __init__(self, hidden_size_lstm, dim_word):\n",
    "        super().__init__()\n",
    "        self.dim_word = dim_word\n",
    "        self.hidden_size_lstm = hidden_size_lstm\n",
    "\n",
    "        # Image embedding (batch_size, 1000) -> (batch_size, hidden_size_lstm)\n",
    "        self.image_embedding = nn.Linear(\n",
    "            in_features=self.dim_word, out_features=self.hidden_size_lstm\n",
    "        )\n",
    "\n",
    "        # LSTM (batch_size, 43, hidden_size_lstm)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.hidden_size_lstm,\n",
    "            hidden_size=self.hidden_size_lstm,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Word back embedding (batch_size, 42, hidden_size_lstm) -> (batch_size, 42, dim_word)\n",
    "        self.word_back_embedding = nn.Linear(\n",
    "            in_features=self.hidden_size_lstm, out_features=self.dim_word\n",
    "        )\n",
    "\n",
    "    def forward(self, image, caption):\n",
    "        # Image embedding (batch_size, 1000) -> (batch_size, 1, hidden_size_lstm)\n",
    "        image = self.image_embedding(image)\n",
    "        image = image.unsqueeze(1)\n",
    "\n",
    "        # Concatenate image and caption (batch_size, 43, hidden_size_lstm)\n",
    "        input_ = torch.cat((image, caption), dim=1)\n",
    "\n",
    "        # LSTM (batch_size, 43, hidden_size_lstm)\n",
    "        output, _ = self.lstm(input_)\n",
    "\n",
    "        # Remove image from caption (batch_size, 42, hidden_size_lstm)\n",
    "        output = output[:, 1:, :]\n",
    "\n",
    "        # Word back embedding (batch_size, 42, hidden_size_lstm) -> (batch_size, 42, dim_word)\n",
    "        output = self.word_back_embedding(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def loss_fn(self, output, target):\n",
    "        return nn.MSELoss()(output, target)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, image, caption = batch\n",
    "\n",
    "        # Encode image and caption\n",
    "        output = self(image, caption)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn(output, caption)\n",
    "\n",
    "        # Log loss\n",
    "        self.log(\"train.loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        learning_rate = 0.01\n",
    "        return torch.optim.SGD(self.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 3: Training und Evaluation \n",
    "Trainiere und evaluiere das Modell. Beschreibe genau was Du tust und warum Du es tust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NIC(1000, 1000)\n",
    "trainer = li.Trainer(\n",
    "    logger=DVCLiveLogger(),\n",
    "    max_epochs=epochs_train,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example prediction\n",
    "file, image, caption = test_dataset[0]\n",
    "output = model(image.unsqueeze(0), caption.unsqueeze(0))\n",
    "\n",
    "# Prepare output\n",
    "output = output.squeeze(0)\n",
    "\n",
    "# reconstruct caption\n",
    "print(\n",
    "    [word2vec.wv.most_similar([word.detach().numpy()], topn=1)[0][0] for word in output]\n",
    ")\n",
    "\n",
    "# get image\n",
    "image = images_dict[file]\n",
    "\n",
    "# show image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(f\"Filename: {file}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 4: Präsentation / Bericht \n",
    "- Präsentation (~10m): Kurze Präsentation mit Diskussion der wichtigsten Ergebnisse. <br> \n",
    "- Q&A (~10min): Klärung von Verständnisfragen zum Paper und der Umsetzung. <br>\n",
    "- Bericht in Form eines gut dokumentierten, übersichtlichen Jupyter Notebooks. \n",
    "\n",
    "Dieses soll schliesslich auch abgegeben werden und dem Fachexperten erlauben, die <br>\n",
    "Schritte nachzuvollziehen (allenfalls auch das Training erneut laufen zu lassen). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beurteilung \n",
    "Beurteilt wird auf Basis des abgegebenen Notebooks:  \n",
    "- Vollständige und korrekte Umsetzung der vereinbarten Aufgabestellung. \n",
    "- Klare, gut-strukturierte Umsetzung.  \n",
    "- Schlüssige Beschreibung und Interpretation der Ergebnisse. Gut gewählte und gut <br> kommentierten Plots und Tabellen. \n",
    "- Vernünftiger Umgang mit (Computing-)Ressourcen. \n",
    "- Verständliche Präsentation der Ergebnisse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referenzen, Key Words \n",
    "- Word Embedding (z.B. word2vec, glove), um Wörter in numerische Vektoren in einem <br> geeignet dimensionierten Raum zu mappen. Siehe z.B. Andrew Ng, Coursera: <br>https://www.coursera.org/lecture/nlp-sequence-models/learning-word-embeddings-APM5s  \n",
    "- Bild Embedding mittels vortrainierten (evt. retrained) Netzwerken wie beispielsweise <br>ResNet, GoogLeNet, EfficientNet oder ähnlich. Transfer-Learning. \n",
    "- Seq2Seq Models bekannt für Sprach-Übersetzung.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
